@article{goldberg2016a,
  author = {Goldberg, Y.},
  date = {2016},
  title = {A Primer on Neural Network Models for Natural Language Processing},
  volume = {57},
  pages = {345–420},
  language = {en},
  journal = {Journal of Artificial Intelligence Research}
}

@Article{mikolov2013a,
  author   = {Mikolov, T. and Sutskever, I. and Chen, K. and Corrado, G.S. and Dean, J.},
  journal  = {Advances in Neural Information Processing Systems (NIPS)},
  title    = {Distributed Representations of Words and Phrases and their Compositionality},
  year     = {2013},
  pages    = {3111–3119},
  date     = {2013},
  language = {en},
}

@Proceedings{pennington2014a,
  title     = {GloVe: Global Vectors for Word Representation},
  author    = {Pennington, J. and Socher, R. and Manning, C.D.},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 1532-1543},
  date      = {2014},
  language  = {en},
}

@Book{jurafsky2019a,
  author    = {Jurafsky, D. and Martin, J.H.},
  publisher = {Pearson},
  title     = {Speech and Language Processing},
  year      = {2019},
  edition   = {3rd},
  date      = {2019},
  language  = {en},
}

@article{collobert2011a,
  author = {Collobert, R. and Weston, J. and Bottou, L. and Karlen, M. and Kavukcuoglu, K. and Kuksa, P.},
  date = {2011},
  title = {Natural Language Processing (Almost) from Scratch},
  volume = {12},
  pages = {2493–2537},
  language = {en},
  journal = {Journal of Machine Learning Research}
}

@Article{mikolov2013b,
  author   = {Mikolov, T. and Chen, K. and Corrado, G. and Dean, J.},
  journal  = {preprint},
  title    = {Efficient Estimation of Word Representations in Vector Space},
  year     = {2013},
  note     = {arXiv preprint arXiv:1301.3781.},
  arxiv    = {1301.3781},
  date     = {2013},
  language = {es},
}

@article{bojanowski2017a,
  author = {Bojanowski, P. and Grave, E. and Joulin, A. and Mikolov, T.},
  date = {2017},
  title = {Enriching Word Vectors with Subword Information},
  volume = {5},
  pages = {135–146},
  language = {en},
  journal = {Transactions of the Association for Computational Linguistics}
}

@Article{bahdanau2015a,
  author   = {Bahdanau, D. and Cho, K. and Bengio, Y.},
  journal  = {preprint},
  title    = {Neural Machine Translation by Jointly Learning to Align and Translate},
  year     = {2015},
  note     = {arXiv preprint arXiv:1409.0473.},
  arxiv    = {1409.0473},
  date     = {2015},
  language = {en},
}

@Proceedings{devlin2019a,
  title     = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  publisher = {Long and Short Papers},
  volume    = {1},
  author    = {Devlin, J. and Chang, M.W. and Lee, K. and Toutanova, K.},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  date      = {2019},
  language  = {en},
  pages     = {4171–4186},
}

@Article{yang2019a,
  author   = {Yang, Z. and Dai, Z. and Yang, Y. and Carbonell, J. and Salakhutdinov, R. and Le, Q.V.},
  journal  = {Advances in Neural Information Processing Systems (NeurIPS)},
  title    = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
  year     = {2019},
  pages    = {5754–5764},
  date     = {2019},
  language = {en},
}

@Article{radford2018a,
  author   = {Radford, A. and Narasimhan, K. and Salimans, T. and Sutskever, I.},
  journal  = {Open AI Research},
  title    = {Improving Language Understanding by Generative Pre-training},
  year     = {2018},
  note     = {Retrieved from},
  date     = {2018},
  language = {en},
  url      = {https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf},
}

@Article{raffel2019a,
  author   = {Raffel, C. and Shazeer, N. and Roberts, A. and Lee, K. and Narang, S. and Matena, M. and Liu, P.J.},
  journal  = {preprint},
  title    = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.},
  year     = {2019},
  note     = {arXiv preprint arXiv:1910.10683.},
  arxiv    = {1910.10683},
  date     = {2019},
  language = {en},
}

@Article{brown2020a,
  author   = {Brown, T.B. and Mann, B. and Ryder, N. and Subbiah, M. and Kaplan, J. and Dhariwal, P. and Amodei, D.},
  journal  = {preprint},
  title    = {Language Models are Few-Shot Learners.},
  year     = {2020},
  note     = {arXiv preprint arXiv:2005.14165.},
  arxiv    = {2005.14165},
  date     = {2020},
  language = {sq},
}

@Article{vaswani2017a,
  author    = {Vaswani, A. and Shazeer, N. and Parmar, N. and Uszkoreit, J. and Jones, L. and Gomez, A.N. and Polosukhin, I.},
  title     = {Attention is All You Need},
  pages     = {5998–6008},
  booktitle = {Advances in Neural Information Processing Systems (NIPS)},
  date      = {2017},
  language  = {en},
}

@Article{liu2020a,
  author   = {Liu, Y. and Ott, M. and Goyal, N. and Du, J. and Joshi, M. and Chen, D. and Stoyanov, V.},
  journal  = {preprint},
  title    = {RoBERTa: A Robustly Optimized BERT Pretraining Approach.},
  year     = {2020},
  note     = {arXiv preprint arXiv:1907.11692.},
  arxiv    = {1907.11692},
  date     = {2020},
  language = {hu},
}

@Article{dosovitskiy2020a,
  author   = {Dosovitskiy, A. and Beyer, L. and Kolesnikov, A. and Weissenborn, D. and Zhai, X. and Unterthiner, T. and Houlsby, N.},
  journal  = {preprint},
  title    = {An Image is Worth 16x16 Words},
  year     = {2020},
  note     = {arXiv preprint arXiv:2010.11929.},
  arxiv    = {2010.11929},
  date     = {2020},
  language = {en},
}

@Comment{jabref-meta: databaseType:bibtex;}
