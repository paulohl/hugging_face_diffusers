	• Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). Language models are unsupervised multitask learners. OpenAI Blog.
	• Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). Attention is all you need. In Advances in neural information processing systems.
	• Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press. (This text provides foundational knowledge on deep learning techniques central to the models discussed.)
	• Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. NAACL-HLT.
•	Brown, T. B., et al. (2020). Language models are few-shot learners. NeurIPS.

