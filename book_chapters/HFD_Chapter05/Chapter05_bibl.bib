@Proceedings{Ruder2019,
  title     = {Transfer Learning in NLP},
  year      = {2019},
  author    = {Ruder, S. and Peters, M. E. and Swayamdipta, S. and Wolf, T.},
  booktitle = {Proceedings of NAACL 2019 Tutorial},
}

@Article{Lee2020,
  author       = {Lee, J.},
  title        = {BioBERT: A pre-trained biomedical language representation model for biomedical text mining},
  number       = {4},
  pages        = {1234, 1240},
  volume       = {36},
  date         = {2020},
  journaltitle = {Bioinformatics},
}

@InProceedings{Ruder2019a,
  author = {Ruder, S. and Peters, M. E. and Swayamdipta, S. and Wolf, T.},
  title  = {Transfer Learning in NLP},
  year   = {2019},
}

@Article{Lee2020a,
  author  = {Lee, J.},
  journal = {Bioinformatics},
  title   = {BioBERT: A pre-trained biomedical language representation model for biomedical text mining},
  year    = {2020},
  pages   = {1234, 1240},
  volume  = {36},
}

@InProceedings{Pires2019,
  author    = {Pires, T. and Schlinger, E. and Garrette, D.},
  title     = {How Multilingual is Multilingual BERT?},
  year      = {2019},
  pages     = {4996–5001},
  publisher = {Association for Computational Linguistics},
}

@Article{Brown2020,
  author  = {Brown, T. and Mann, B. and Ryder, N. and Subbiah, M. and Kaplan, J. D. and Dhariwal, P. and Neelakantan, A. and Shyam, P. and Sastry, G. and Askell, A. and Agarwal, S. and Herbert-Voss, A. and Krueger, G. and Henighan, T. and Child, R. and Ramesh, A. and Ziegler, D. M. and Wu, J. and Winter, C. and Amodei, D.},
  journal = {Advances in Neural Information Processing Systems},
  title   = {Language Models are Few-Shot Learners},
  year    = {2020},
  pages   = {1877–1901},
  volume  = {33},
}

@Article{Sanh2019,
  author = {Sanh, V and , Debut, L. and Chaumond, J. and Wolf, T.},
  title  = {DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  year   = {2019},
}

@Article{Chalkidis2020,
  author = {Chalkidis, I. and Fergadiotis, M. and Malakasiotis, P. and Aletras, N. and Androutsopoulos, I.},
  title  = {LEGAL-BERT: The Muppets straight out of law school},
  year   = {2020},
}

@Article{Srivastava2014,
  author  = {Srivastava, N. and Hinton, G. and Krizhevsky, A. and Sutskever, I. and Salakhutdinov, R.},
  journal = {Journal of Machine Learning Research},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  year    = {2014},
  pages   = {1929–1958},
  volume  = {15},
}

@Article{Zhang2015,
  author    = {Zhang, X and , Zhao, J. and LeCun, Y.},
  journal   = {Advances in Neural Information Processing Systems},
  title     = {Character-level Convolutional Networks for Text Classificatio},
  year      = {2015},
  pages     = {649–657},
  volume    = {28},
  publisher = {NeurIPS},
}

@Article{Liu2019,
  author = {Liu, Y. and Ott, M. and Goyal, N. and Du, J. and Joshi, M. and Chen, D. and Levy, O. and Lewis, M. and Zettlemoyer, L. and Stoyanov, V.},
  title  = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  year   = {2019},
}

@Article{Raffel2020,
  author = {Raffel, C.},
  title  = {Exploring the limits of transfer learning with a unified text-to-text transformer},
  year   = {2020},
  note   = {Preprint arXiv:1910.10683.},
  arxiv  = {1910.10683},
}

@Article{Howard2018,
  author = {Howard, J. and Ruder, S.},
  title  = {Universal language model fine-tuning for text classification},
  year   = {2018},
  note   = {Preprint arXiv:1801.06146.},
  arxiv  = {1801.06146},
}

@Article{Devlin2019,
  author = {Devlin, J. and Chang, M.-W. and Lee, K. and Toutanova, K.},
  title  = {BERT: Pre-training of deep bidirectional transformers for language understanding},
  year   = {2019},
  note   = {Preprint arXiv:1810.04805.},
  arxiv  = {1810.04805},
}

@Article{Conneau2020,
  author = {Conneau, A.},
  title  = {Unsupervised cross-lingual representation learning at scale},
  year   = {2020},
  note   = {Preprint arXiv:1911.02116.},
  arxiv  = {1911.02116},
}

@Comment{jabref-meta: databaseType:bibtex;}
